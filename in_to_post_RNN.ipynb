{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stack-Augmented Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and relevant dependencies\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import string\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Dyck library\n",
    "from tasks.dyck_generator import DyckLanguage\n",
    "\n",
    "# RNN Models\n",
    "from models.rnn_models import VanillaRNN, SRNN_Softmax, SRNN_Softmax_Temperature, SRNN_GumbelSoftmax\n",
    "\n",
    "# Set default tensor type \"double\"\n",
    "torch.set_default_tensor_type('torch.DoubleTensor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fix the random seed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "RANDOM SEED: 23\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x116194b88>"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "randomseed_num = 23\n",
    "print ('RANDOM SEED: {}'.format(randomseed_num))\n",
    "random.seed (randomseed_num)\n",
    "np.random.seed (randomseed_num)\n",
    "torch.manual_seed(randomseed_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPU/CPU Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU/CPU Check\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") ## GPU stuff\n",
    "print (device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dyck Languages (Don't run this section if not using)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A probabilistic context-free grammar for $\\mathcal{D}_n$ can be written as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "S \\rightarrow \\begin{cases} \n",
    "(_i\\, S\\, )_i & \\text{with probability } \\frac{p}{n} \\\\\n",
    "S\\,S & \\text{with probability } q \\\\ \n",
    "\\varepsilon & \\text{with probability } 1 - (p+q) \n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "where $0 < p, q < 1$ and $p+q < 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Test Corpora Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training corpus window : [`MIN_SIZE`, `MAX_SIZE`]\n",
    "\n",
    "Test corpus window: [`MAX_SIZE+2`, `2*MAX_SIZE`]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters of the Probabilistic Dyck Language \n",
    "NUM_PAR = 2\n",
    "MIN_SIZE = 2\n",
    "MAX_SIZE = 50\n",
    "P_VAL = 0.5\n",
    "Q_VAL = 0.25\n",
    "\n",
    "# Number of samples in the training corpus\n",
    "TRAINING_SIZE = 5000\n",
    "# Number of samples in the test corpus\n",
    "TEST_SIZE = 5000\n",
    "\n",
    "# Create a Dyck language generator\n",
    "Dyck = DyckLanguage (NUM_PAR, P_VAL, Q_VAL)\n",
    "all_letters = word_set = Dyck.return_vocab ()\n",
    "n_letters = vocab_size = len (word_set)\n",
    "\n",
    "print('Loading data...')\n",
    "\n",
    "training_input, training_output, st = Dyck.training_set_generator (TRAINING_SIZE, MIN_SIZE, MAX_SIZE)\n",
    "test_input, test_output, st2 = Dyck.training_set_generator (TEST_SIZE, MAX_SIZE + 2, 2 * MAX_SIZE)\n",
    "\n",
    "for i in range (1):\n",
    "    print (training_output[i])\n",
    "    print (Dyck.lineToTensor(training_output[i]))\n",
    "    print (Dyck.lineToTensorSigmoid(training_output[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infix to Postfix Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of samples in the training corpus\n",
    "TRAINING_SIZE = 5000\n",
    "# Number of samples in the test corpus\n",
    "TEST_SIZE = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stack:\n",
    "    def __init__(self):\n",
    "        self.items = []\n",
    "\n",
    "    def isEmpty(self):\n",
    "        return self.items == []\n",
    "\n",
    "    def push(self, item):\n",
    "        self.items.append(item)\n",
    "\n",
    "    def pop(self):\n",
    "        return self.items.pop()\n",
    "\n",
    "    def update(self, item):\n",
    "        self.items[len(self.items) - 1] = item\n",
    "        return self.items\n",
    "\n",
    "    def peek(self):\n",
    "        return self.items[len(self.items) - 1]\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dataset.tsv.txt'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c45c69aff8be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dataset.tsv.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mTRAINING_SIZE\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mTEST_SIZE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataset.tsv.txt'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"dataset.tsv.txt\", sep=\"\\t\",header=None)[0:TRAINING_SIZE+TEST_SIZE]\n",
    "df[0] = df[0].str.replace(\" \",\"\") \n",
    "df[1] = df[1].str.replace(\" \",\"\")\n",
    "df = df.drop(columns=[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infixToPostfix(infixexpr):\n",
    "    axns_ohe = []\n",
    "    axns_str = []\n",
    "    \n",
    "    prec = {}\n",
    "    prec[\"*\"] = 3\n",
    "    prec[\"/\"] = 3\n",
    "    prec[\"+\"] = 2\n",
    "    prec[\"-\"] = 2\n",
    "    prec[\"(\"] = 1\n",
    "    opStack = Stack()\n",
    "    postfixList = []\n",
    "    tokenList = [char for char in infixexpr]\n",
    "\n",
    "    for token in tokenList:\n",
    "#         print(token, end=\" \")\n",
    "        token_vec = [0, 0, 0]  # push, pop, no-op\n",
    "        token_str = \"\"\n",
    "        if token not in prec and token != \")\":\n",
    "            postfixList.append(token)\n",
    "            token_vec[2] += 1\n",
    "            token_str += \"2\"\n",
    "        elif token == '(':\n",
    "            opStack.push(token)\n",
    "            token_vec[0] += 1\n",
    "            token_str += \"0\"\n",
    "        elif token == ')':\n",
    "            topToken = opStack.pop()\n",
    "            token_vec[1] += 1\n",
    "            token_str += \"1\"\n",
    "            while topToken != '(':\n",
    "                postfixList.append(topToken)\n",
    "                topToken = opStack.pop()\n",
    "#                 token_vec[1] += 1\n",
    "        else:\n",
    "            while (not opStack.isEmpty()) and (prec[opStack.peek()] >= prec[token]):\n",
    "                postfixList.append(opStack.pop())\n",
    "                token_vec[1] += 1\n",
    "            opStack.push(token)\n",
    "            token_vec[0] += 1\n",
    "            token_str += \"0\"\n",
    "        axns_ohe.append(token_vec)\n",
    "        axns_str.append(token_str)\n",
    "    \n",
    "    while not opStack.isEmpty():\n",
    "        postfixList.append(opStack.pop())\n",
    "        \n",
    "    return \"\".join(axns_str), axns_ohe, \"\".join(postfixList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ohe_axn(df):\n",
    "    axn_list_list = []\n",
    "    for item in df.index:\n",
    "        axn_list_list.append(infixToPostfix(df.iloc[item, 0])[0])\n",
    "        \n",
    "    return axn_list_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eqn_vocab = ['(', ')', '*', '+', '-', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "axn_vocab = [\"0\", \"1\", \"2\"]\n",
    "\n",
    "def lineToTensor(line, n_letters, vocab):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][0][vocab.index(letter)] = 1.0\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'lineToTensor' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-0149c5d6755a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlineToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"0002021021021\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxn_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxn_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'lineToTensor' is not defined"
     ]
    }
   ],
   "source": [
    "lineToTensor(\"0002021021021\", len(axn_vocab), axn_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lineToTensor(\"(((9*9)/9)-9)\", len(eqn_vocab), eqn_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-e19fd3ca32df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"axn_list\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mohe_axn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df[\"axn_list\"] = ohe_axn(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the input and output in the same way as the dyck languages\n",
    "training_input, training_output = df[:TRAINING_SIZE][0].tolist(), df[:TRAINING_SIZE][\"axn_list\"].tolist()\n",
    "test_input, test_output = df[TRAINING_SIZE:TRAINING_SIZE+TEST_SIZE][0].tolist(), df[TRAINING_SIZE:TRAINING_SIZE+TEST_SIZE][\"axn_list\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(training_input), len(test_input))\n",
    "print(len(training_output), len(test_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stack-RNN  Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of hidden units\n",
    "n_hidden = 8\n",
    "# Number of hidden layers\n",
    "n_layers = 1\n",
    "# Stack size\n",
    "stack_size = 104\n",
    "stack_dim = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stack-RNN with Softmax\n",
    "# model = SRNN_Softmax (n_hidden, vocab_size, vocab_size, n_layers, stack_size, stack_dim).to(device)\n",
    "# model = VanillaRNN(n_hidden, vocab_size, vocab_size).to(device)  # works with dyck\n",
    "model = VanillaRNN(n_hidden, len(axn_vocab), len(eqn_vocab)).to(device)  # works with predicting stack actions\n",
    "# model = SRNN_Softmax (n_hidden, vocab_size, vocab_size, n_layers, stack_size, stack_dim).to(device)\n",
    "\n",
    "# Learning rate\n",
    "learning_rate = .01\n",
    "# Minimum Squared Error (MSE) loss\n",
    "criterion = nn.MSELoss() \n",
    "# Adam optimizer (https://arxiv.org/abs/1412.6980)\n",
    "optim = torch.optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Model details:')\n",
    "print (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of epochs to train our model\n",
    "epochs = 2\n",
    "# Output threshold\n",
    "epsilon = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing the Stack-RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model (model, data_input, data_output, which):\n",
    "    # Turn on the eval mode\n",
    "    model.eval()\n",
    "    # Total number of \"correctly\" predicted samples\n",
    "    correct_num = 0\n",
    "    with torch.no_grad():\n",
    "        for i in range (len(data_output)):\n",
    "            len_input = len (data_input[i])\n",
    "            model.zero_grad ()\n",
    "            # Initialize the hidden state\n",
    "            hidden = model.init_hidden()\n",
    "            # Initialize the stack\n",
    "            stack = torch.zeros (stack_size, stack_dim).to(device)\n",
    "            # Target values\n",
    "            if which == \"train\":\n",
    "                target = lineToTensor(training_output[i], len(axn_vocab), axn_vocab).to(device) \n",
    "            else:\n",
    "                target = lineToTensor(test_output[i], len(axn_vocab), axn_vocab).to(device) \n",
    "            # Output values\n",
    "            output_vals = torch.zeros (target.shape)\n",
    "            \n",
    "            for j in range (len_input):\n",
    "                if which == \"train\":\n",
    "                    output, hidden, stack = model (lineToTensor(training_input[i][j], len(eqn_vocab), eqn_vocab).to(device), hidden, stack)\n",
    "                else:\n",
    "                    output, hidden, stack = model (lineToTensor(test_input[i][j], len(eqn_vocab), eqn_vocab).to(device), hidden, stack)\n",
    "                output_vals [j] = output\n",
    "\n",
    "            # Binarize the entries based on the output threshold\n",
    "            out_np = np.int_(output_vals.detach().numpy() >= epsilon)\n",
    "            target_np = np.int_(target.detach().numpy())\n",
    "            \n",
    "            # (Double-)check whether the output values and the target values are the same\n",
    "            if np.all(np.equal(out_np, target_np)) and (out_np.flatten() == target_np.flatten()).all():\n",
    "                # If so, increase `correct_num` by one\n",
    "                correct_num += 1\n",
    "                \n",
    "    return float(correct_num)/len(data_output) * 100, correct_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train (model, optimizer, criterion, epoch_num=2):\n",
    "    # Turn on the train model for the model\n",
    "    model.train()\n",
    "    # Arrays for loss and \"moving\" accuracy per epoch\n",
    "    loss_arr = []\n",
    "    correct_arr = []\n",
    "    for epoch in range(1, epoch_num + 1):\n",
    "        print ('Epoch: {}'.format(epoch))\n",
    "        \n",
    "        # Total loss per epoch\n",
    "        total_loss = 0\n",
    "        # Total number of \"correctly\" predicted samples so far in the epoch\n",
    "        counter = 0\n",
    "\n",
    "        for i in range (TRAINING_SIZE):\n",
    "            len_input = len (training_input[i])\n",
    "            # Good-old zero grad\n",
    "            model.zero_grad ()\n",
    "            # Initialize the hidden state\n",
    "            hidden = model.init_hidden()\n",
    "            # Initialize the stack \n",
    "            stack = torch.zeros (stack_size, stack_dim).to(device)\n",
    "            # Target values\n",
    "            target = lineToTensor(training_output[i], len(axn_vocab), axn_vocab).to(device) \n",
    "            # Output values\n",
    "            output_vals = torch.zeros (target.shape)\n",
    "\n",
    "            for j in range (len_input):\n",
    "                output, hidden, stack = model (lineToTensor(training_input[i][j], len(eqn_vocab), eqn_vocab).to(device), hidden, stack)\n",
    "                output_vals [j] = output\n",
    "            \n",
    "            # MSE (y, y_bar)\n",
    "            loss = criterion (output_vals, target)\n",
    "            # Add the current loss to the total loss\n",
    "            total_loss += loss.item()\n",
    "            # Backprop! \n",
    "            loss.backward ()\n",
    "            optimizer.step ()\n",
    "            \n",
    "            # Print the performance of the model every 500 steps\n",
    "            if i % 250 == 0:\n",
    "                print ('Sample Number {}: '.format(i))\n",
    "                print ('Input : {}'.format(training_input[i]))\n",
    "                print ('Output: {}'.format(training_output[i]))\n",
    "                print ('* Counter: {}'.format(counter))\n",
    "                print ('* Avg Loss: {}'.format(total_loss/(i+1))) \n",
    "\n",
    "            # Binarize the entries based on the output threshold\n",
    "            out_np = np.int_(output_vals.detach().numpy() >= epsilon)\n",
    "            target_np = np.int_(target.detach().numpy())\n",
    "                \n",
    "            # \"Moving\" training accuracy\n",
    "            if np.all(np.equal(out_np, target_np)) and (out_np.flatten() == target_np.flatten()).all():\n",
    "                counter += 1\n",
    "                \n",
    "            # At the end of the epoch, append our total loss and \"moving\" accuracy\n",
    "            if i == TRAINING_SIZE - 1:\n",
    "                print ('Counter: {}'.format(float(counter)/TRAINING_SIZE))\n",
    "                loss_arr.append (total_loss)\n",
    "                correct_arr.append(counter) \n",
    "\n",
    "        if epoch % 1 == 0:\n",
    "            print ('Training Accuracy %: ', correct_arr)\n",
    "            print ('Loss: ', loss_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let there be light!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train (model, optim, criterion, epoch_num=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Performance of the Stack-RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set accuracy \n",
    "correct_num = test_model (model, training_input, training_output, \"train\")\n",
    "print ('Training accuracy: {}.'.format(correct_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set accuracy \n",
    "correct_num = test_model (model, test_input, test_output, \"test\")\n",
    "print ('Test accuracy: {}.'.format(correct_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save/Upload the Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model weights\n",
    "torch.save(model.state_dict(), 'models/vanilla_rnn_model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the model weights\n",
    "# model.load_state_dict(torch.load('models/stack_rnn_model_weights.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Stack Elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.graph_objs import *\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode,  iplot, plot\n",
    "import plotly.graph_objs as go\n",
    "import math\n",
    "\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def enable_plotly_in_cell():\n",
    "  import IPython\n",
    "  from plotly.offline import init_notebook_mode\n",
    "  display(IPython.core.display.HTML('''\n",
    "        <script src=\"/static/components/requirejs/require.js\"></script>\n",
    "  '''))\n",
    "  init_notebook_mode(connected=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Hidden State and Stack Configuration for a Given Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hidden_and_stack_info (model, input, output):\n",
    "    # Turn on the evaluation mode for the model\n",
    "    model.eval()\n",
    "    # Hidden state values\n",
    "    hidden_states = []\n",
    "    # Stack configuration\n",
    "    stack_config = []\n",
    "    # Stack operation weights\n",
    "    operation_weights = []\n",
    "    # Most recently pushed element to the stack\n",
    "    new_elt_inserted = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        len_input = len (input)\n",
    "        model.zero_grad ()\n",
    "        \n",
    "        # Initialize the hidden state\n",
    "        hidden = model.init_hidden()\n",
    "        # Initalize the stack configuration\n",
    "        stack = torch.zeros (stack_size, stack_dim).to(device)\n",
    "        # Target values\n",
    "        target = Dyck.lineToTensorSigmoid(output)\n",
    "        # Output values\n",
    "        output_vals = torch.zeros (target.shape).to(device)\n",
    "\n",
    "        for j in range (len_input):\n",
    "            # Feed the input to the model\n",
    "            output, hidden, stack = model (Dyck.lineToTensor(input[j]).to(device), hidden, stack)\n",
    "            # Hidden state values\n",
    "            hidden_states.append (hidden.cpu().numpy())\n",
    "            # Stack configuration \n",
    "            stack_config.append (stack.cpu().numpy())\n",
    "            # Stack operation weights\n",
    "            operation_weights.append (model.action_weights.cpu().numpy())\n",
    "            # New element inserted to the stack\n",
    "            new_elt_inserted.append (model.new_elt.cpu().numpy())\n",
    "            # Output value\n",
    "            output_vals [j] = output.view(-1)\n",
    "        \n",
    "        # Binarize the entries based on the output threshold\n",
    "        out_np = np.int_(output_vals.cpu().detach().numpy() >= epsilon)\n",
    "        target_np = np.int_(target.cpu().detach().numpy())\n",
    "        \n",
    "        # (Double-)check whether the output values and the target values are the same\n",
    "        if np.all(np.equal(out_np, target_np)) and (out_np.flatten() == target_np.flatten()).all():\n",
    "            print ('Correct!')\n",
    "        else:\n",
    "            print ('Incorrect')\n",
    "            \n",
    "    return hidden_states, stack_config, operation_weights, new_elt_inserted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq = '([])[[[((([[(())]])))]]][()]'\n",
    "output_seq =  Dyck.output_generator (input_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states, stack_config, operation_weights, new_elt_inserted = get_hidden_and_stack_info (model, input_seq, output_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Stack Operation Weights at Each Timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_stack_operation_weights (operation_weights, input_seq, timestep=0):\n",
    "    # Stack operation labels\n",
    "    labels = ['PUSH', 'POP']\n",
    "    stack_op_weights = np.squeeze(operation_weights)\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    fig = sns.heatmap(stack_op_weights.T, cmap=sns.light_palette(\"#34495e\"),xticklabels=input_seq, yticklabels=labels, vmin=0, vmax=1)\n",
    "    fig.set_title('Strength of Stack Operations at Each Timestep', fontsize=17)\n",
    "    cbar = fig.collections[0].colorbar\n",
    "    cbar.set_ticks(np.linspace(0,1,6))\n",
    "    plt.xlabel('Sequence', fontsize=16)\n",
    "    plt.ylabel('Actions', fontsize=16)\n",
    "    plt.xticks(fontsize=13)\n",
    "    plt.yticks(fontsize=14)\n",
    "    plt.show()\n",
    "    # plt.savefig('stackrnn_weights', dpi=128, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_stack_operation_weights (operation_weights, input_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Stack Configuration at Each Timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_stack_configuration (stack_config, input, dimension=0):\n",
    "    stack_bound = 13 #len (input)\n",
    "    print (np.array(stack_config).shape)\n",
    "    print (stack_bound)\n",
    "    stack_config = np.round(np.array(stack_config)[:, :stack_bound+1, dimension], decimals=3)\n",
    "    location = np.arange (1, stack_bound+2)\n",
    "    plt.figure(figsize=(18, 12))\n",
    "    fig = sns.heatmap(stack_config.T, cmap='viridis', yticklabels = location, xticklabels=input, annot=True, cbar=False)\n",
    "    fig.invert_yaxis()\n",
    "    fig.set_title('Stack Entries at Each Timestep', fontsize=17)\n",
    "    plt.xticks(fontsize=13)\n",
    "    plt.yticks(fontsize=13)\n",
    "    plt.xlabel('Sequence', fontsize=16)\n",
    "    plt.ylabel('Stack Location', fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_stack_configuration (stack_config, input_seq, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Hidden State Values at Each Timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_hidden_states (hidden_states, input):\n",
    "    plt.style.use('default')\n",
    "    domain = list(range(len(input)))\n",
    "    hidden_states = np.squeeze(hidden_states).T\n",
    "    for i in range (int(n_hidden/2)):\n",
    "        plt.figure()\n",
    "        for j in range (2):\n",
    "            plt.plot (domain, hidden_states[i*2+j], label='Unit {}'.format(i*2+j+1))\n",
    "        plt.legend (loc='upper right')\n",
    "        plt.xticks(domain, input_seq) \n",
    "        plt.title ('Analysis of the Hidden State Dynamics')\n",
    "        plt.ylabel ('Activation Values')\n",
    "        plt.ylim (-1.15, 1.15, 10)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_hidden_states (hidden_states, input_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q.E.D."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}